# -*- coding: utf-8 -*-
"""PoAIt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PQMSWMQ_9MVSzj7r8ytXKC_81QxYUDiO
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import pandas as pd
tf.__version__

txt_df = pd.read_csv('../../content/train2.csv',header=None)
txt_df.head(3)

txt_list=txt_df.values.tolist()
bow = []
for i,txt in enumerate(txt_list):
    p_txt = txt[0].replace('\n',' \n ').replace('\t',' \t ').split(' ')
    bow.extend(p_txt)

print(bow)
print(len(bow))

vocab=sorted(set(bow))

import numpy as np
word2idx = {u:i for i,u in enumerate(vocab)} #{'아':3}
idx2word = np.array(vocab)

text_as_int = np.array([word2idx[c] for c in bow ]) # text to int

print('Unique words : {}'.format(len(vocab)))

for char,_ in zip(word2idx,range(20)):
    print('{}:{}'.format(repr(char),word2idx[char]))
text = bow

print('{} ---- mapping to int --> {}'.format(repr(text[20:30]),text_as_int[20:30]))

len(text)/30

seq_length = 30
examples_per_epoch = len(text)/seq_length

char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

for i in char_dataset.take(20):
    print(idx2word[i.numpy()])

sequences = char_dataset.batch(seq_length+1, drop_remainder=True)# tf.data.Dataset.from_tensor_slices.batch 개수 남으면 버리기  
# seq_length+1  batch to  trainset 1~100   testset 2~101

for item in sequences.take(5):
  print(repr(' '.join(idx2word[item.numpy()])))

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text,target_text

dataset = sequences.map(split_input_target) 
print(dataset)

dataset

for input_example,target_example in dataset.take(1):
    print('input data:',repr(''.join(idx2word[input_example.numpy()])))
    print('output data:',repr(''.join(idx2word[target_example.numpy()])))

for i,(input_idx,target_idx) in enumerate(zip(input_example[:5],target_example[:5])):
    print("{:4d} iter".format(i))
    print(" input: {} ({:s})".format(input_idx,idx2word[input_idx]))
    print(" prediction: {} ({:s})".format(target_idx,idx2word[target_idx]))

BATCH_SIZE = 32
BUFFER_SIZE = 100000
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)

dataset

vocab_size = len(vocab)

embedding_dim = 1024

rnn_units = 128

def build_model(vocab_size,embedding_dim,rnn_units,batch_size):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),
        tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),
        tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),
        tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])
    return model

model = build_model(vocab_size=vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE)

BATCH_SIZE

for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape)

model.summary()

sampled_indices = tf.random.categorical(example_batch_predictions[0],1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()
sampled_indices

print('input: \n',repr(' '.join(idx2word[input_example_batch[0].numpy()])))
print('\nprediction for next letter: \n',repr(' '.join(idx2word[sampled_indices])))

def loss(labels,logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True) #softmax

example_batch_loss = loss(target_example_batch,example_batch_predictions)
print(example_batch_predictions.shape)
print(example_batch_loss.numpy().mean())

import os
model.compile(loss=loss,optimizer='Adam')
checkpoint_dir='./training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir,"ckpt_{epoch}")

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)



EPOCHS=10
history = model.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])

tf.train.latest_checkpoint(checkpoint_dir)

model2 = build_model(vocab_size,embedding_dim,rnn_units,batch_size=1)
model2.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model2.build(tf.TensorShape([1, None]))
model2.summary()

def generate_text(model,start_string=None):
    num_generate = 60
    
    # input_eval = word2idx[start_string] 
    # input_eval = word2idx[[np.random.randint(100,15000)]]
    # input_eval = tf.expand_dims([np.random.randint(100,15000)],0)
    input_eval = tf.expand_dims([10058],0)

    text_generated=[]
    temperature = 0.9
    

    model.reset_states() #stateful = True 로 생성된 초기 state 초기화
    for i in range(num_generate):
        
        predictions = model(input_eval)
        
        predictions = tf.squeeze(predictions,0)

        predictions = predictions/temperature  # reduce greedy
        predicted_id = tf.random.categorical(predictions,1)[-1,0].numpy()
        
        input_eval = tf.expand_dims([predicted_id],0)

        text_generated.append(idx2word[predicted_id])

    return (start_string+'\n\n'+' '.join(text_generated))

print(generate_text(model2,'\t\t무제'))

idx2word[10014]