# -*- coding: utf-8 -*-
"""PoAIt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-P6njtLJmFDsY9l0KqGsuIa99hILHsJ
"""

import pandas as pd

idx_to_morp_df = pd.read_csv('../../content/idx_to_morp.csv')
morp_to_idx_df = pd.read_csv('../../content/morp_to_idx.csv')
morp_txt_df = pd.read_csv('../../content/morp_txt.csv')
morp_vect_df = pd.read_csv('../../content/morp_vect.csv')

import pandas as pd
import numpy as np
import tensorflow as tf

idx_to_morp={i:u for i,u in idx_to_morp_df.values}
morp_to_idx = {u:i for i,u in idx_to_morp_df.values}

len(idx_to_morp)

morp_txt = morp_txt_df.values.squeeze()

morp_vect = morp_vect_df.values.squeeze()

len(morp_vect)

dataset = tf.data.Dataset.from_tensor_slices(morp_vect)

sequence= 60
sequences = dataset.batch(sequence+1, drop_remainder=True)

def split_input_chunk(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text,target_text
dataset = sequences.map(split_input_chunk)

batch_size = 32
buffer_size = 120000
dataset_batch = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

dataset_batch

# for input_example_batch, target_example_batch in dataset_batch.take(1):
#   example_batch_predictions = model(input_example_batch)
#   print(example_batch_predictions.shape, "# (배치 크기, 시퀀스 길이, 어휘 사전 크기)")

vocab_size = len(idx_to_morp)
embedding_dim = 820
rnn_units = 480

# embedding_dim = 1024
# rnn_units = 512

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(17673,embedding_dim,batch_input_shape=[batch_size,None]))
model.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))
# model.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=False,recurrent_initializer='glorot_uniform'))
# model.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=False,recurrent_initializer='glorot_uniform'))
model.add(tf.keras.layers.Dense(vocab_size))
model.summary()

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)

history = model.fit(dataset_batch, epochs=200,metrics=['acc'])

model.save_weights('../../content/model_weigths.h5')

model2 = tf.keras.Sequential()
model2.add(tf.keras.layers.Embedding(17673,embedding_dim,batch_input_shape=[1,None]))
model2.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))
# model.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=False,recurrent_initializer='glorot_uniform'))
# model.add(tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=False,recurrent_initializer='glorot_uniform'))
model2.add(tf.keras.layers.Dense(17673))
model2.summary()

model2.load_weights('../../content/drive/My Drive/model/weights')

def generate_text(model2, temp):
  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)

  # 생성할 문자의 수
  num_generate = 120

  # 시작 문자열을 숫자로 변환(벡터화)
#   input_eval = [char2idx[s] for s in start_string]
#   input_eval = tf.expand_dims(input_eval, 0)
  input_eval = np.array([np.random.randint(0,17670)])
  input_eval=np.expand_dims(input_eval,axis=0)
  # 결과를 저장할 빈 문자열
  text_generated = []

  # 온도가 낮으면 더 예측 가능한 텍스트가 됩니다.
  # 온도가 높으면 더 의외의 텍스트가 됩니다.
  # 최적의 세팅을 찾기 위한 실험
  temperature = temp

  # 여기에서 배치 크기 == 1
  model2.reset_states()
  for i in range(num_generate):
      predictions = model2(input_eval)
      # 배치 차원 제거
      predictions = tf.squeeze(predictions, 0)

      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # 예측된 단어를 다음 입력으로 모델에 전달
      # 이전 은닉 상태와 함께
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx_to_morp[predicted_id])

  return (' '.join(text_generated))

# print(generate_text(model2,1))
# print('---------')
# print(generate_text(model2,0.5))
# print('---------')
# print(generate_text(model2,1.2))
# print('---------')
# print(generate_text(model2,1))

# print('\t\t\t\t\t무 제\n\n',generate_text(model2,1.6))
while 1:
    a = generate_text(model2,1.6)
    if '\n\n' in a:
        print('\t\t\t\t\t무 제\n\n',a)
        break

model2.save('Generator.h5')



